{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import itertools as it\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as selection\n",
    "import os\n",
    "import json\n",
    "import pickle as pic\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"./../utils\")\n",
    "sys.path.append(\"./../sklearn\")\n",
    "import util\n",
    "import constants as const\n",
    "import ColumnTransformer\n",
    "importlib.reload(util)\n",
    "importlib.reload(const)\n",
    "importlib.reload(ColumnTransformer)\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('precision', 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world graphs, R-MAT and Albert Barabsi\n",
    "combined_graphs_encoded = pd.read_csv(\"../data/combined.csv\")\n",
    "# Wiki graphs for enrichment\n",
    "graphs_for_enrichment = pd.read_csv(\"../data/enrichment.csv\")\n",
    "# The used partitioners\n",
    "partitioners = list(combined_graphs_encoded.partitioner.unique())\n",
    "print(partitioners)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"SVR\"\n",
    "\n",
    "TARGETS = [\n",
    "    \"vertex_balance\", \n",
    "    \"destination_balance\", \n",
    "    \"source_balance\", \n",
    "    \"edge_balance\", \n",
    "    \"replication_factor\"\n",
    "    ]\n",
    "for TARGET in TARGETS:\n",
    "    base_mode = partitioners + [\n",
    "        'num_partitions', \n",
    "        \"pearson_mode_degrees_in\",\n",
    "        \"pearson_mode_degrees_out\",\n",
    "        \"mean_degree\",\n",
    "        \"density\",\n",
    "    ]\n",
    "    easy_mode = [base_mode]\n",
    "    hard_mode = [base_mode + [\"mean_triangles\", \"average_lcc\"]]\n",
    "\n",
    "    all_feature_sets=[\n",
    "        easy_mode, \n",
    "        hard_mode\n",
    "    ]\n",
    "    \n",
    "    all_feature_sets_description=[\n",
    "        \"Easy\", \n",
    "        \"Hard\"\n",
    "    ]\n",
    "    RESULT = {}\n",
    "\n",
    "    for feature_index in range(len(all_feature_sets)):\n",
    "        print(\"#######################################################################################################################\")\n",
    "        print(\"We are starting with features:\", all_feature_sets[feature_index])\n",
    "        print(\"#######################################################################################################################\")\n",
    "\n",
    "        if (not TARGET == \"replication_factor\") and all_feature_sets_description[feature_index] == \"Hard\":\n",
    "            print(\"We do not train\")\n",
    "            continue\n",
    "        \n",
    "        model = util.get_svr( \n",
    "            features=all_feature_sets[feature_index],\n",
    "            kernels=[\"linear\"],\n",
    "            interval_C= np.linspace(0.001, 1, 5),\n",
    "            interval_epsilon=np.linspace(0.001, 0.1, 5)\n",
    "    )\n",
    "\n",
    "        rmat_types = [\n",
    "            \"rmat-medium\", \n",
    "            \"rmat-small\",\n",
    "        ]\n",
    "\n",
    "        real_world_types = [\n",
    "             'realworld-web', \n",
    "             'realworld-internet', \n",
    "             'realworld-interaction',    \n",
    "             'realworld-soc', \n",
    "             'realworld-product_network',\n",
    "             'realworld-collaboration', \n",
    "             'realworld-citation',\n",
    "             'realworld-cummunication',\n",
    "             'realworld-affiliation',\n",
    "             'realworld-wiki'\n",
    "\n",
    "        ]\n",
    "\n",
    "        data_train_validate_test = [] # [(X_train, X_val, X_test, y_train, y_val, y_test)]\n",
    "        data_description = []\n",
    "        enrich_by = []\n",
    "\n",
    "        for _enrich_by in [0.0, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8, 1.0]:\n",
    "        #for _enrich_by in [0.0]:\n",
    "            rmat_on_real = [\n",
    "                {\"graph_type\": rmat_types, \"training_size\": 0.8, \"validation_size\": 0.2, \"test_size\": 0 },\n",
    "                {\"graph_type\": real_world_types, \"training_size\": 0, \"validation_size\": 0, \"test_size\": 1 },\n",
    "                {\"enrich_by\": _enrich_by },\n",
    "            ]   \n",
    "            data_train_validate_test.append(util.get_train_validate_test(\n",
    "                combined_graphs_encoded, \n",
    "                graphs_for_enrichment,\n",
    "                target=TARGET,\n",
    "                configurations=rmat_on_real)) \n",
    "            data_description.append(\"rmat-enriched-with-real\")\n",
    "            enrich_by.append(_enrich_by)\n",
    "\n",
    "        validation_results, test_results = util.train(\n",
    "            model=model, \n",
    "            data_train_validate_test=data_train_validate_test, \n",
    "            data_description=data_description, \n",
    "            enrich_by=enrich_by,\n",
    "            used_feature_set=all_feature_sets_description[feature_index], \n",
    "            target=TARGET,\n",
    "            model_name=MODEL_NAME)\n",
    "\n",
    "        \n",
    "        RESULT[all_feature_sets_description[feature_index]+\"-validation\"] = validation_results\n",
    "        RESULT[all_feature_sets_description[feature_index]+\"-test\"] = test_results\n",
    "        \n",
    "    for scores in RESULT.keys():\n",
    "        print(\"store\", scores)\n",
    "        RESULT[scores].to_csv(\"../models/{}_{}_{}\".format(MODEL_NAME, scores, TARGET))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
