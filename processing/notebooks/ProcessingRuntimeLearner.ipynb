{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import itertools as it\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as selection\n",
    "import os\n",
    "import json\n",
    "import pickle as pic\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"./../utils\")\n",
    "sys.path.append(\"./../sklearn\")\n",
    "import util\n",
    "import constants as const\n",
    "import ColumnTransformer\n",
    "importlib.reload(util)\n",
    "importlib.reload(const)\n",
    "importlib.reload(ColumnTransformer)\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('precision', 6)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import json\n",
    "import pickle\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Some Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_partitioners = [\n",
    "    '1dd', \n",
    "    '1ds', \n",
    "    '2d', \n",
    "    '2ps', \n",
    "    'crvc', \n",
    "    'dbh', \n",
    "    'hdrf',\n",
    "    'hep1',\n",
    "    'hep10', \n",
    "    'hep100', \n",
    "    'ne',\n",
    "] \n",
    "\n",
    "\n",
    "TARGET = \"processing_time\"\n",
    "target = \"processing_time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, we make copies. We will later filter. \n",
    "X_train = pd.read_csv(\"../datasets/graph-processing-run-time_train.csv\")\n",
    "y_train = X_train.copy()\n",
    "# Yes, we make copies. We will later filter. \n",
    "X_test = pd.read_csv(\"../datasets/graph-processing-run-time_test.csv\")\n",
    "y_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper\n",
    "## 2.1 For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, algorithm, model, X_test, y_test, target):\n",
    "    \"\"\" We evalute the given model on the testset. \n",
    "\n",
    "    Args:\n",
    "        model_name (string): the name of the model, e.g., XGB for extrem gradient boosting\n",
    "        algorithm (string): the graph processing algorithm, e.g., cc, pr, sssp1, ...\n",
    "        model (model): the trained model\n",
    "        X_test (test data features): the testdata\n",
    "        y_test (test date target): the testdata\n",
    "        features (list): the features used in the model\n",
    "        target (string): the name if the target column\n",
    "    Returns:\n",
    "        list of dicts: the scores evaluated by partitioner and by graph \n",
    "    \"\"\"\n",
    "    \n",
    "    graphs = X_test.graph.unique()\n",
    "    partitioners = X_test.partitioner.unique()\n",
    "\n",
    "    results = []\n",
    "    # 1. Get scores by partitioner and per test graph. This for loop also works if we only have on partitioner.\n",
    "    for graph in graphs:\n",
    "        for partitioner in partitioners:\n",
    "            _X_test = X_test.copy()\n",
    "            _y_test = y_test.copy()\n",
    "            \n",
    "            # Only evaluate the current graph on the current partitioner\n",
    "            _X_test = _X_test[(_X_test.graph == graph)  & (_X_test.partitioner == partitioner) ]\n",
    "            _y_test = _y_test[(_y_test.graph == graph) & (_y_test.partitioner == partitioner)]\n",
    "\n",
    "            # The predicted processing time\n",
    "            _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "\n",
    "            # The true processing time\n",
    "            _X_test[target] = _y_test[target]\n",
    "\n",
    "            # Get the metrics/scores.\n",
    "            metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "            result = metrics.get_metrics(_X_test)\n",
    "            \n",
    "            result[\"partitioner\"] = partitioner\n",
    "            result[\"graph\"] = graph.split(\"/\")[-1]\n",
    "            result[\"model\"] = model_name\n",
    "            result[\"algorithm\"] = algorithm\n",
    "            results.append(result)\n",
    "\n",
    "    # Since we also use this evaluation method for a model trained only for one single partitioner. \n",
    "    # In the next line we will evaluate how well the model performed for the different partitioners on the given graph processing algorithm: One model multiple partitioners and one graph processing algorithm.\n",
    "    if (len(partitioners) > 1):\n",
    "        # 2. Get result by partitioner and per test grap\n",
    "        for partitioner in partitioners:\n",
    "            _X_test = X_test.copy()\n",
    "            _y_test = y_test.copy()\n",
    "            \n",
    "            # Only evaluate the current graph on the current partitioner\n",
    "            _X_test = _X_test[(_X_test.partitioner == partitioner)]\n",
    "            _y_test = _y_test[(_y_test.partitioner == partitioner)]\n",
    "\n",
    "            # The predicted processing time\n",
    "            _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "\n",
    "            # The true processing time\n",
    "            _X_test[target] = _y_test[target]\n",
    "\n",
    "            metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "            result = metrics.get_metrics(_X_test)\n",
    "\n",
    "            result[\"partitioner\"] = partitioner\n",
    "            result[\"graph\"] = \"all\"\n",
    "            result[\"model\"] = model_name\n",
    "            result[\"algorithm\"] = algorithm\n",
    "            results.append(result)\n",
    "\n",
    "    \n",
    "        # 3. Get the overalle score for all partitoiners on all graphs for the fiven graph processing algorithm\n",
    "        _X_test = X_test.copy()\n",
    "        _y_test = y_test.copy()\n",
    "    \n",
    "        _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "        _X_test[target] = _y_test[target]\n",
    "\n",
    "\n",
    "        metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "        result = metrics.get_metrics(_X_test)\n",
    "        result[\"partitioner\"] = \"all\"\n",
    "        result[\"graph\"] = \"all\"\n",
    "        result[\"model\"] = model_name\n",
    "        result[\"algorithm\"] = algorithm\n",
    "        results.append(result)\n",
    "    \n",
    "    #IF We only train with one partitioner, then we can not set partitioner to \"all\"\n",
    "    else:\n",
    "        # 3. Get for the one partitioner the scores. In this else we only have one partitioner which will be at index 0\n",
    "        partitioner = partitioners[0]\n",
    "        _X_test = X_test.copy()\n",
    "        _y_test = y_test.copy()\n",
    "    \n",
    "        _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "        _X_test[target] = _y_test[target]\n",
    "\n",
    "\n",
    "        metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "        result = metrics.get_metrics(_X_test)\n",
    "        result[\"partitioner\"] = partitioner\n",
    "        result[\"graph\"] = \"all\"\n",
    "        result[\"model\"] = model_name\n",
    "        result[\"algorithm\"] = algorithm\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name,algorithm, model, X_train, y_train, X_test, y_test, feature_combinations, target, store_model=False, result_file_name=\"\"):\n",
    "    print(\"Trainer will store model in \", result_file_name)\n",
    "    \"\"\"Traing the given pipeline on the training data and evaluate the model on the test set. \n",
    "\n",
    "    Args:\n",
    "        model_name (string): name of the model, e.g., rfr for random forest regressor\n",
    "        model (pipeline): the pipeline to traing\n",
    "        X_train (_type_): train data - features\n",
    "        y_train (_type_): train data - target\n",
    "        X_test (_type_): test data - features\n",
    "        y_test (_type_): test data - target\n",
    "        target (_type_): the target to predict\n",
    "\n",
    "    Returns:\n",
    "        list: the achieved scores\n",
    "    \"\"\"\n",
    "    _X_train = X_train.copy()\n",
    "    _y_train = y_train.copy()\n",
    "    _X_test = X_test.copy()\n",
    "    _y_test = y_test.copy()\n",
    "\n",
    "    model.fit(_X_train, _y_train[target])\n",
    "    print(\"PARAMETERS\", model.get_params().keys())\n",
    "    print(\"Best params\", model.best_params_)\n",
    "    print(\"Mean Test score\", model.cv_results_[\"mean_test_score\"])\n",
    "    \n",
    "    # Now we have the trained model\n",
    "    # We store all the cross validation scores and also the best parameters\n",
    "    # We further directly evaluation on the test set and also store these scores. \n",
    "    # IMPORTANT: Later on, we need to make the model selection based on the crossvalidation scores, not on the test set. \n",
    "\n",
    "    # We serialize and store the best model     \n",
    "    if (store_model and not \"\" == result_file_name):\n",
    "        pickle.dump(model, open(result_file_name, 'wb'))\n",
    "\n",
    "    # Get the scores. \n",
    "    scores = evaluate_model(model_name, algorithm, model, X_test=_X_test, y_test=_y_test, target=target)\n",
    "    # Yes, for each score we add the cv score on the rmat graphs, the best hyperparameters. \n",
    "    # Because even if we evaluate on different graphs/partitioners, these score do not differ because they are based on synthetic graphs and lead to the model selection  \n",
    "    for score in scores:\n",
    "        score[\"rmat_best_params\"] = model.best_params_\n",
    "        score[\"rmat_cv_score\"] =  model.cv_results_[\"mean_test_score\"][model.best_index_]\n",
    "        score[\"rmat_cv_scores\"] =  model.cv_results_[\"mean_test_score\"]\n",
    "        score[\"rmat_best_index\"] = model.best_index_\n",
    "    return scores\n",
    "   # print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training used features\n",
    "## 3.1 Used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_properties_simple = [\n",
    "  'num_edges',\n",
    "  'num_vertices',\n",
    "]\n",
    "\n",
    "featuresets = []\n",
    "for graph_properties in [\n",
    "  graph_properties_simple, \n",
    "  ]:\n",
    "  quality_features = helper.create_Feature_combinations(\n",
    "    [\n",
    "      \"vertex_balance\",\t\n",
    "      \"destination_balance\",\t\n",
    "      \"source_balance\",\t\n",
    "      \"replication_factor\",\n",
    "      \"edge_balance\",\n",
    "    ], \n",
    "    [])   \n",
    "  for quality_feature in quality_features:\n",
    "    features = quality_feature + graph_properties\n",
    "    featuresets.append(features)\n",
    "print(\"Our features set consits of\", len(featuresets), \" combinations\\nfeatures: \", featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_scores = []\n",
    "PATH_STORE_MODEL = helper.create_results_folder(\"/home/ubuntu/cephstorage/graph-processing-run-time-prediction-models\")\n",
    "PATH_STORE_SCORES = helper.create_results_folder(\"results-graph-processing-run-time-prediction\")\n",
    "\n",
    "for USE_ALL_PARTITIONER in [\n",
    "    True, \n",
    "  ]:\n",
    "  for algo in [\n",
    "    \"cc\", \n",
    "    \"pr\",\n",
    "    \"sssp1\", \n",
    "    \"kcoreavg\",\n",
    "    \"synthetic1c0\",\n",
    "    \"synthetic10c0\"\n",
    "    ]:\n",
    "    for model_name in [\n",
    "      \"KNN\",\n",
    "      \"PolyRegression\",\n",
    "      \"XGB\", \n",
    "      \"RFR\", \n",
    "      \"SVR\"\n",
    "      ]:\n",
    "      print(\"We start the training with model ({}) for algorithm ({}). We build ONE model for all partitioners: {}\".format(model_name, algo, USE_ALL_PARTITIONER))\n",
    "      if USE_ALL_PARTITIONER:\n",
    "        training_approach = \"single-model-for-all-partitioners\"\n",
    "        # We use all the partitioners for the paper\n",
    "        _X_train =  X_train[(X_train.partitioner.isin(paper_partitioners)) &(X_train.algorithm == algo)]\n",
    "        _y_train =  y_train[(y_train.partitioner.isin(paper_partitioners)) &(y_train.algorithm == algo)]\n",
    "        _X_test =  X_test[(X_test.partitioner.isin(paper_partitioners)) &(X_test.algorithm == algo)]\n",
    "        _y_test =  y_test[(y_test.partitioner.isin(paper_partitioners)) &(y_test.algorithm == algo)]\n",
    "     #   print(\"_X_train with {} samples. {} are expected\".format(len(_X_train), NUM_TRAIN_GRAPHS * NUM_PARTITIONERS))\n",
    "     #   print(\"_y_train with {} samples. {} are expected\".format(len(_y_train), NUM_TRAIN_GRAPHS * NUM_PARTITIONERS))\n",
    "     #   print(\"_X_test with {} samples. {} are expected\".format(len(_X_test), NUM_TEST_GRAPHS * NUM_PARTITIONERS))\n",
    "     #   print(\"_y_test with {} samples. {} are expected\".format(len(_y_test), NUM_TEST_GRAPHS * NUM_PARTITIONERS))\n",
    "        # We get the model which includes the gridsearch and pipeline\n",
    "        model = helper.get_model(model_name, featuresets)\n",
    "        # Get scores: Will include that crossvalidation score for the best hyperparameters and also the testscore for the best hyperparameters. \n",
    "        # The testscores will be 1) per partitioner and per graph 2) per graph 3) per partitioner 4) overall score\n",
    "        scores = train(\n",
    "          model_name=model_name, \n",
    "          algorithm=algo, \n",
    "          model=model, \n",
    "          X_train=_X_train, \n",
    "          y_train=_y_train, \n",
    "          X_test=_X_test, \n",
    "          y_test=_y_test, \n",
    "          feature_combinations=featuresets, \n",
    "          target=TARGET, \n",
    "          store_model=True,\n",
    "          result_file_name=\"{}/{}_{}_{}\".format(PATH_STORE_MODEL, training_approach, algo, model_name))\n",
    "\n",
    "\n",
    "        for score in scores:\n",
    "          score[\"algorithm\"] = algo\n",
    "          score[\"training_approach\"] = training_approach\n",
    "        overall_scores += scores\n",
    "        pd.DataFrame(scores).to_csv(\"{}/{}_{}_{}\".format(PATH_STORE_SCORES, algo, training_approach, model_name))\n",
    "      else:\n",
    "        training_approach = \"single-model-per-partitioner\"\n",
    "        for partitioner in paper_partitioners:\n",
    "            _X_train =  X_train[(X_train.partitioner == partitioner) &(X_train.algorithm == algo)]\n",
    "            _y_train =  y_train[(y_train.partitioner == partitioner)&(y_train.algorithm == algo)]\n",
    "            _X_test =  X_test[(X_test.partitioner == partitioner)&(X_test.algorithm == algo)]\n",
    "            _y_test =  y_test[(y_test.partitioner == partitioner)&(y_test.algorithm == algo)]\n",
    "        #    print(\"_X_train with {} samples. {} are expected\".format(len(_X_train), NUM_TRAIN_GRAPHS ))\n",
    "        #    print(\"_y_train with {} samples. {} are expected\".format(len(_y_train), NUM_TRAIN_GRAPHS ))\n",
    "        #    print(\"_X_test with {} samples. {} are expected\".format(len(_X_test),  NUM_TEST_GRAPHS))\n",
    "        #    print(\"_y_test with {} samples. {} are expected\".format(len(_y_test), NUM_TEST_GRAPHS))\n",
    "            # We get the model which includes the gridsearch and pipeline\n",
    "            model = helper.get_model(model_name, featuresets)\n",
    "            # Get scores: Will include that crossvalidation score for the best hyperparameters and also the testscore for the best hyperparameters. \n",
    "            # The testscores will be 1) per partitioner and per graph 2) per graph 3) per partitioner 4) overall score\n",
    "            scores = train(\n",
    "              model_name=model_name, \n",
    "              algorithm=algo , \n",
    "              model=model, \n",
    "              X_train=_X_train, \n",
    "              y_train=_y_train, \n",
    "              X_test=_X_test,\n",
    "              y_test=_y_test, \n",
    "              feature_combinations=featuresets,\n",
    "              target=TARGET,\n",
    "              store_model=True,\n",
    "              result_file_name=\"{}/{}_{}_{}_{}\".format(PATH_STORE_MODEL, partitioner, training_approach, algo, model_name))\n",
    "            for score in scores:\n",
    "              score[\"algorithm\"] = algo\n",
    "              score[\"training_approach\"] = training_approach\n",
    "            overall_scores += scores\n",
    "            pd.DataFrame(scores).to_csv(\"{}/{}_{}_{}_{}\".format(PATH_STORE_SCORES, training_approach, algo, partitioner, model_name))\n",
    "\n",
    "scores_df = pd.DataFrame(overall_scores)\n",
    "scores_df.to_csv(\"{}/scores-{}\".format(PATH_STORE_SCORES, TARGET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluations\n",
    "Have a look at the run-time-prediction-scores.ipynb with the scores reported in the paper \n",
    "\n",
    "set the right timestamp!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(\"{}/scores-processing_time\".format(PATH_STORE_SCORES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_overall_df = scores_df[(scores_df.graph == \"all\") & (scores_df.partitioner == \"all\") & (scores_df.training_approach == \"single-model-for-all-partitioners\")  ]\n",
    "print(\"We trained {} models but select the best one based on crossvalidation on the rmat graphs\".format(len(scores_overall_df)))\n",
    "\n",
    "idx = scores_overall_df.groupby([\"algorithm\", \"partitioner\"])['rmat_cv_score'].transform(max) == scores_overall_df['rmat_cv_score']\n",
    "best_scores_overall_df = scores_overall_df[idx].sort_values(by=[\"algorithm\", \"mape\"])\n",
    "print(\"One average we can predict the grap processing run-time with a mape of {}\".format(best_scores_overall_df[\"mape\"].mean()))\n",
    "best_scores_overall_df[[\"r2\", \"rmse\", \"mape\", \"partitioner\", \"graph\", \"model\", \"algorithm\", \"rmat_cv_score\", \"training_approach\", \"rmat_best_params\"]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
