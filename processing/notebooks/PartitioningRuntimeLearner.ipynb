{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import itertools as it\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as selection\n",
    "import os\n",
    "import json\n",
    "import pickle as pic\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"./../utils\")\n",
    "sys.path.append(\"./../sklearn\")\n",
    "import util\n",
    "import constants as const\n",
    "import ColumnTransformer\n",
    "importlib.reload(util)\n",
    "importlib.reload(const)\n",
    "importlib.reload(ColumnTransformer)\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('precision', 6)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Some Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_partitioners = [\n",
    "    '1dd', \n",
    "    '1ds', \n",
    "    '2d', \n",
    "    '2ps', \n",
    "    'crvc', \n",
    "    'dbh', \n",
    "    'hdrf',\n",
    "    'hep1',\n",
    "    'hep10', \n",
    "    'hep100', \n",
    "    'ne',\n",
    "] \n",
    "\n",
    "TARGET = \"partitioning_time\"\n",
    "target = \"partitioning_time\"\n",
    "num_parts = [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, we make copies. We will later filter. \n",
    "X_train = pd.read_csv(\"../datasets/graph-partitioning-run-time_train.csv\")\n",
    "y_train = X_train.copy()\n",
    "# Yes, we make copies. We will later filter. \n",
    "X_test = pd.read_csv(\"../datasets/graph-partitioning-run-time_test.csv\")\n",
    "y_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper\n",
    "## 2.1 Helper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model, X_test, y_test, target):\n",
    "    \"\"\" We evalute the given model on the testset. \n",
    "    Args:\n",
    "        model_name (string): the name of the model, e.g., XGB\n",
    "        model (model): the trained model\n",
    "        X_test (test data features): the testdata\n",
    "        y_test (test data target): the testdata\n",
    "        target (string): the name if the target column: \"partitioning_time\"\n",
    "    Returns:\n",
    "        list of dicts: the scores evaluated by partitioner and by graph \n",
    "    \"\"\"\n",
    "    # Which graphs are contained in the test set\n",
    "    graphs = X_test.graph.unique()\n",
    "    # Which partitioners are contained in the test set\n",
    "    partitioners = X_test.partitioner.unique()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 1. Get scores by partitioner and per test graph. This for loop also works if we only have on partitioner.\n",
    "    for graph in graphs:\n",
    "        for partitioner in partitioners:\n",
    "            _X_test = X_test.copy()\n",
    "            _y_test = y_test.copy()\n",
    "            \n",
    "            # Only evaluate the current graph on the current partitioner\n",
    "            _X_test = _X_test[(_X_test.graph == graph)  & (_X_test.partitioner == partitioner)]\n",
    "            _y_test = _y_test[(_y_test.graph == graph) & (_y_test.partitioner == partitioner)]\n",
    "\n",
    "            # The predicted partitioning time\n",
    "            _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "\n",
    "            # Set the true partitioning time\n",
    "            _X_test[target] = _y_test[target]\n",
    "            \n",
    "            # Get the metrics/scores.\n",
    "            metrics = helper.Metrics(target=target, predicted_target=\"predicted_{}\".format(target))\n",
    "            result = metrics.get_metrics(_X_test)\n",
    "\n",
    "            result[\"partitioner\"] = partitioner\n",
    "            result[\"graph\"] = graph.split(\"/\")[-1]\n",
    "            result[\"model\"] = model_name\n",
    "            \n",
    "            results.append(result)\n",
    "\n",
    "    # Since we also use this evaluation method for a model trained only for one single partitioner. \n",
    "    # In the next line we will evaluate how well the model performed for the different partitioner: One model multiple partitioners. \n",
    "    if (len(partitioners) > 1):\n",
    "        # In this if we evaluate our model which we trained for different partitioners, not only for one partitioner.\n",
    "\n",
    "        # 2. Get Score for each partitioner\n",
    "        for partitioner in partitioners:\n",
    "            _X_test = X_test.copy()\n",
    "            _y_test = y_test.copy()\n",
    "            \n",
    "            # Only evaluate the current graph on the current partitioner\n",
    "            _X_test = _X_test[(_X_test.partitioner == partitioner)]\n",
    "            _y_test = _y_test[(_y_test.partitioner == partitioner)]\n",
    "\n",
    "            # The predicted partitioning time\n",
    "            _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "\n",
    "            # The true partitioning time\n",
    "            _X_test[target] = _y_test[target]\n",
    "\n",
    "            metrics = helper.Metrics(target=target, predicted_target=\"predicted_{}\".format(target))\n",
    "            result = metrics.get_metrics(_X_test)\n",
    "            result[\"partitioner\"] = partitioner\n",
    "            result[\"graph\"] = \"all\" # because the partitioner is evaluated on all graphs.\n",
    "            result[\"model\"] = model_name\n",
    "            results.append(result)\n",
    "\n",
    "        # 3. Get the overall score for all partitioners on all graphs for the given model\n",
    "        _X_test = X_test.copy()\n",
    "        _y_test = y_test.copy()\n",
    "\n",
    "        # The predicted partitioning time\n",
    "        _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "        _X_test[target] = _y_test[target]\n",
    "\n",
    "        metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "        result = metrics.get_metrics(_X_test)\n",
    "        result[\"partitioner\"] = \"all\"\n",
    "        result[\"graph\"] = \"all\"\n",
    "        result[\"model\"] = model_name\n",
    "        results.append(result)\n",
    "    \n",
    "    # IF We only train with one partitioner, then we can not set partitioner to \"all\". \n",
    "    else:\n",
    "        # 3. Get for the one partitioner the scores. In this else we only have one partitioner which will be at index 0\n",
    "        partitioner = partitioners[0]\n",
    "        _X_test = X_test.copy()\n",
    "        _y_test = y_test.copy()\n",
    "    \n",
    "        _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "        _X_test[target] = _y_test[target]\n",
    "\n",
    "        metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "        result = metrics.get_metrics(_X_test)\n",
    "        result[\"partitioner\"] = partitioner\n",
    "        result[\"graph\"] = \"all\"\n",
    "        result[\"model\"] = model_name\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper for Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, model, X_train, y_train, X_test, y_test, target, store_model=False, result_file_name=\"\"):\n",
    "    \"\"\"Traing the given pipeline on the training data and evaluate the model on the test set.\n",
    "\n",
    "    Args:\n",
    "        model_name (string): name of the model, e.g., rfr for random forest regressor\n",
    "        model (pipeline): the pipeline to train. IT includes the grid search\n",
    "        X_train (_type_): train data - features\n",
    "        y_train (_type_): train data - target\n",
    "        X_test (_type_): test data - features\n",
    "        y_test (_type_): test data - target\n",
    "        target (_type_): the target to predict: \"partitioning_time\"\n",
    "        store_model (boolean): whether the best model in repsect to the achieved cross validation score should be saved\n",
    "        result_file_name (string): where to store the model if it should be stored.\n",
    "\n",
    "    Returns:\n",
    "        list: the achieved scores\n",
    "    \"\"\"\n",
    "    _X_train = X_train.copy()\n",
    "    _y_train = y_train.copy()\n",
    "    _X_test = X_test.copy()\n",
    "    _y_test = y_test.copy()\n",
    "\n",
    "    # Train the model incl. hyperparamter search. The hyperparameters are selected ONLY on the cross-validation score on the synthetic graphs.\n",
    "    model.fit(_X_train, _y_train[target])\n",
    "\n",
    "    print(\"PARAMETERS\", model.get_params().keys())\n",
    "    print(\"Best params\", model.best_params_)\n",
    "    print(\"Mean Test score\", model.cv_results_[\"mean_test_score\"])\n",
    "    \n",
    "    # Now we have the trained model\n",
    "    # We store all the cross validation scores and also the best hyper-parameters\n",
    "    # We further directly evaluate on the test set and also store these scores. \n",
    "    # IMPORTANT: Later on, we need to make the model selection / Hyperparamter search based on the crossvalidation scores, not on the test set of course. \n",
    "\n",
    "    # We serialize and store the best model\n",
    "    if (store_model and not \"\" == result_file_name):\n",
    "        pickle.dump(model, open(result_file_name, 'wb'))\n",
    "\n",
    "    # Get the scores. \n",
    "    scores = evaluate_model(model_name, model, X_test=_X_test, y_test=_y_test, target=target)\n",
    "\n",
    "    # Yes, for each score we add the cv score on the rmat graphs, the best hyperparameters. \n",
    "    # Because even if we evaluate on different graphs/partitioners, these score do not differ because the are based on synthetic graphs and lead to the model selection  \n",
    "    for score in scores:\n",
    "        score[\"rmat_best_params\"] = model.best_params_\n",
    "        score[\"rmat_cv_score\"] =  model.cv_results_[\"mean_test_score\"][model.best_index_]\n",
    "        score[\"rmat_cv_scores\"] =  model.cv_results_[\"mean_test_score\"]\n",
    "        score[\"rmat_best_index\"] = model.best_index_\n",
    "    return scores\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_properties_simple = [\n",
    "  'num_edges',\n",
    "  'num_vertices',\n",
    "]\n",
    "\n",
    "graph_properties_easy = graph_properties_simple +  [\n",
    "  \"pearson_mode_degrees_in\",\n",
    "  \"pearson_mode_degrees_out\",\n",
    "  \"mean_degree\",\n",
    "  \"density\",\n",
    "]  \n",
    "\n",
    "graph_properties_hard = graph_properties_easy + [\n",
    "    \"mean_triangles\", \n",
    "    \"average_lcc\",\n",
    "]\n",
    "\n",
    "featuresets = [\n",
    "  graph_properties_simple, \n",
    "  graph_properties_easy, \n",
    "  graph_properties_hard, \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_scores = []\n",
    "PATH_STORE_MODEL = helper.create_results_folder(\"/home/ubuntu/cephstorage/partitioning-run-time-prediction-models\")\n",
    "PATH_STORE_SCORES = helper.create_results_folder(\"results-partitioning-run-time-prediction\")\n",
    "\n",
    "for USE_ALL_PARTITIONER in [\n",
    "  True, \n",
    "  False\n",
    "  ]:\n",
    "  for model_name in [\n",
    "    \"KNN\",\n",
    "    \"PolyRegression\",\n",
    "    \"XGB\", \n",
    "    \"RFR\", \n",
    "    \"SVR\"\n",
    "    ]:\n",
    "    print(\"We start the training with model ({}). We build ONE model for all partitioners: {}\".format(model_name, USE_ALL_PARTITIONER))\n",
    "    if USE_ALL_PARTITIONER:\n",
    "\n",
    "      # We need to add the partitioner as a feature. Our trainings data already is one hot encoded but we need to allow the model to used the columns\n",
    "      partitioner_one_hot_columns = paper_partitioners\n",
    "      _featuresets = []\n",
    "      for fs in featuresets:\n",
    "        _featuresets.append(fs + partitioner_one_hot_columns)\n",
    "      training_approach = \"single-model-for-all-partitioners\"\n",
    "\n",
    "      # We use all the partitioners for the paper\n",
    "      _X_train =  X_train[(X_train.partitioner.isin(paper_partitioners)) ]\n",
    "      _y_train =  y_train[(y_train.partitioner.isin(paper_partitioners)) ]\n",
    "      _X_test =  X_test[(X_test.partitioner.isin(paper_partitioners)) ]\n",
    "      _y_test =  y_test[(y_test.partitioner.isin(paper_partitioners)) ]\n",
    "      #print(\"_X_train with {} samples. {} are expected\".format(len(_X_train), NUM_TRAIN_GRAPHS * NUM_PARTITIONERS))\n",
    "      #print(\"_y_train with {} samples. {} are expected\".format(len(_y_train), NUM_TRAIN_GRAPHS * NUM_PARTITIONERS))\n",
    "      #print(\"_X_test with {} samples. {} are expected\".format(len(_X_test), NUM_TEST_GRAPHS * NUM_PARTITIONERS))\n",
    "      #print(\"_y_test with {} samples. {} are expected\".format(len(_y_test), NUM_TEST_GRAPHS * NUM_PARTITIONERS))\n",
    "      # We get the model which includes the gridsearch and pipeline\n",
    "      model = helper.get_model(model_name, _featuresets)\n",
    "      # Get scores: Will include that crossvalidation score for the best hyperparameters and also the scores for the test set\n",
    "      # The testscores will be 1) per partitioner and per graph 2) per graph 3) per partitioner 4) overall score\n",
    "      scores = train(\n",
    "        model_name=model_name, \n",
    "        model=model, \n",
    "        X_train=_X_train, \n",
    "        y_train=_y_train, \n",
    "        X_test=_X_test, \n",
    "        y_test=_y_test, \n",
    "        target=TARGET, \n",
    "        store_model=True,\n",
    "        result_file_name=\"{}/{}_{}\".format(PATH_STORE_MODEL, training_approach, model_name))\n",
    "      for score in scores:\n",
    "        score[\"training_approach\"] = training_approach\n",
    "      overall_scores += scores\n",
    "      pd.DataFrame(scores).to_csv(\"{}/{}_{}\".format(PATH_STORE_SCORES, training_approach, model_name))\n",
    "    \n",
    "    else:\n",
    "      training_approach = \"single-model-per-partitioner\"\n",
    "      # For each partitioner we train a model \n",
    "      for partitioner in paper_partitioners:\n",
    "          _X_train =  X_train[(X_train.partitioner == partitioner) ]\n",
    "          _y_train =  y_train[(y_train.partitioner == partitioner)]\n",
    "          _X_test =  X_test[(X_test.partitioner == partitioner)]\n",
    "          _y_test =  y_test[(y_test.partitioner == partitioner)]\n",
    "         # print(\"_X_train with {} samples. {} are expected\".format(len(_X_train), NUM_TRAIN_GRAPHS))\n",
    "         # print(\"_y_train with {} samples. {} are expected\".format(len(_y_train),NUM_TRAIN_GRAPHS))\n",
    "         # print(\"_X_test with {} samples. {} are expected\".format(len(_X_test), NUM_TEST_GRAPHS))\n",
    "         # print(\"_y_test with {} samples. {} are expected\".format(len(_y_test), NUM_TEST_GRAPHS))\n",
    "          # We get the model which includes the gridsearch and pipeline\n",
    "          model = helper.get_model(model_name, featuresets)\n",
    "          # Get scores: Will include that crossvalidation score for the best hyperparameters and also the scores for the test set. \n",
    "          scores = train(\n",
    "            model_name=model_name,  \n",
    "            model=model, \n",
    "            X_train=_X_train, \n",
    "            y_train=_y_train, \n",
    "            X_test=_X_test,\n",
    "            y_test=_y_test, \n",
    "            target=TARGET,\n",
    "            store_model=True,\n",
    "            result_file_name=\"{}/{}_{}_{}\".format(PATH_STORE_MODEL,training_approach, partitioner, model_name))\n",
    "          for score in scores:\n",
    "            score[\"training_approach\"] = training_approach\n",
    "          overall_scores += scores\n",
    "          pd.DataFrame(scores).to_csv(\"{}/{}_{}_{}\".format(PATH_STORE_SCORES, training_approach, partitioner, model_name))\n",
    "\n",
    "scores_df = pd.DataFrame(overall_scores)\n",
    "scores_df.to_csv(\"{}/scores-{}\".format(PATH_STORE_SCORES, TARGET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "Have a look at the run-time-prediction-scores.ipynb with the scores reported in the paper\n",
    "\n",
    "Use the correct time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(\"{}/scores-partitioning_time\".format(PATH_STORE_SCORES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selction based on rmat cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well performed the model on average if evaluted for all graphs and all partitioners\n",
    "scores_overall_df = scores_df[(scores_df.graph == \"all\") & (scores_df.partitioner == \"all\") & (scores_df.training_approach == \"single-model-for-all-partitioners\")]\n",
    "NUMBER_MODELS = len(scores_overall_df.model.unique())\n",
    "print(\"We trained {} models ({} expected) but select the best one based on crossvalidation on the rmat graphs\".format(len(scores_overall_df), NUMBER_MODELS))\n",
    "#Based on the cross valiudation score we select the best model. \n",
    "idx = scores_overall_df.groupby([\"partitioner\"])['rmat_cv_score'].transform(max) == scores_overall_df['rmat_cv_score']\n",
    "# Now, we report the mape for the best model selected in the line above.\n",
    "best_scores_overall_df = scores_overall_df[idx]\n",
    "#print(\"MAPE: \", best_scores_overall_df[\"mape\"].mean())\n",
    "best_scores_overall_df[[\"r2\", \"rmse\", \"mape\", \"partitioner\", \"graph\", \"model\", \"rmat_cv_score\", \"training_approach\", \"rmat_best_params\"]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
